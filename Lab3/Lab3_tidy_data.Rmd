---
title: 'Bio 201: Lab 3 tidy data'
author: "Kristi Gdanetz MacCready"
date: "9/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/UMich_Bio201_F19/Lab3/")
```

# Load packages
Packages in R are basically sets of additional functions that let you do more stuff. The functions you’ve been using so far are part of R (also called base install) and no extra action is needed to utilize these functions; packages give you access to more functions. Before you use a package for the first time you need to install it on your machine. Then any subsequent R sessions where the package will be used require an import step at the beginning of the session. The tidyverse package should already be installed on the USB lab computers. This is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, ggplot2, tibble, etc.
 
The tidyverse package tries to address 3 common issues that arise when doing data analysis with some of the functions that come with R:
 
The results from a base R function sometimes depend on the type of data.
Using R expressions in a non standard way, which can be confusing for new learners.
Hidden arguments, having default operations that new learners are not aware of.

There are several other useful packages that are part of the tidyverse or are written to work well with the tidyverse. Install and load these packages: readxl, broom, cowplot. 

```{r Load packages, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
library(tidyverse)
library(readxl)
library(broom)
library(cowplot)
set.seed(7)
```

# Loading data

There are several methods to read in the SCFA measurement data. We could use the readr() package, which is loaded as part of the tidyverse. This package has functions for reading in specific file formats; general delimited files (read_delim), tab separated values (read_tsv), comma separated values files (read_csv), fixed width files (read_fwf), and files where columns are separated by whitespace (read_table). We are also going to use the read_excel function from the readxl package to read a table in from a Microsoft Excel-formatted spreadsheet since Excel spreadsheets are a common way collaborates share data.  
 
Each of these functions has a decent number of options that default to values that are generally intuitive. Be careful - there are other similarly named functions (e.g. read.tsv) that are actually part of base R and have somewhat unexpected defaults. What are the defaults of these functions? What options can you change? Remember from the last lesson that you can view the documentation for any function. 

### with readr
The output of the read functions that are part of the tidyverse are a special type of data frame called a tibble. To back up a step, what is a data frame? A data frame can be thought of as a table where each row represents a different entity and each column represents a different aspect of that entity. For example, the scfa_wkly variable stores the value of a data frame where each row represents a different person and each column represents various attributes of those people such as their participant identification number, weight, height, location, diagnosis, smoking status, etc. Each row has the same number of columns. If a piece of data is missing, then R will denote the value for that entity with the NA value. Got it? In summary, a tibble is a special type of data frame that is a stripped down version of the data.frame structure that is core to R. Keeping with the . for _ theme, data_frame can be used as an alias for tibble.
 
There are some special aspects of a tibble to be aware of. Perhaps most important is that there are no names on the rows. Absence of row names is a safety measure to protect you from some weird quirks in R. Another difference is when you enter the name of the data frame at the prompt, instead of having the entire data frame vomited at your screen, you get an abbreviated output.

```{r}
scfa_wkly <- read_delim(file = "raw_data/SCFA_wkly.txt", 
                        delim = "\t", escape_double = FALSE, trim_ws = TRUE, na=c("NA"),
                        col_types = list())
scfa_wkly
``` 

The abbreviated output above gives the first several columns and the first ten rows of the data frame. You’ll notice that at the bottom of the output, it tells us additional rows and columns. In addition, the output tells us the variable type of each column.

```{r}
# remove import data frame from global environment 
rm(scfa_wkly)
```

### Import with excel files

We'll repeat the import using a different function, and review the output. 
```{r}
scfa_indv <- read_excel("raw_data/SCFA_data.xlsx",
                        sheet = "SCFA_indv", col_names = TRUE, trim_ws = TRUE, 
                        na = c("", "NA", "-----")) 
# type into the console: scfa_indv
```

The output in the console is similar to what we created above, we can see the dimensions (rows and columns) and type of each variable. For example, the tube_wt column contains dbl or double precision numbers and the Participant_ID column contains chr or character values. You’ll also notice that zero values have a lighter color and any NAs are red. These features are all meant to improve the visualization of the data.

This format is referred to as a “tibble”. Tibbles tweak some of the default behaviors of data frame objects. You can think of a data frame as several vectors group together into a spreadsheet. Accessing or modifying individual columns of a tibble or data frame is very similar to the manipulations on vectors. Going forward we will almost exclusively be working with tibbles/data frames. 


# Exploring data

Yay, finally! Let’s dig into the data to think about how we’d like to use it to answer our research questions. Whenever you read in a data frame, there are a few things to complete to get a handle on your data. First, as we’ve already done, entering the name of the data frame at the prompt will tell us a lot of information. We might also want to access individual chunks of those data. 

### Summary functions
The functions below can provide useful information about a data frame. Run each on the data frame `scfa_indv`, and read the help pages if necessary. In the comments briefly describe what each function tells you about the data frame. 

```{r eval=FALSE, include=FALSE}
nrow() #
ncol() #
dim() #
colnames() #
rownames() #
glimpse() #
str() #
```

### Column names

Look again at the column names in the scfa_indv data frame, notice that some names are in title case (e.g., “Participant_ID”) and others are in all lower case (e.g., “tube_wt”). Also, some of the column names may not make sense if you are not familiar with the types of SCFAs measured. Some of the column names contain spaces, while others use underscores. Let’s fix these issues to make using the data easier. 

```{r}
colnames(scfa_indv)
```

There are two problems with the name “tube_wt”; the capitalization is inconsistent with the other columns and it may not be immediately clear what “tube_wt” means. We have many options for how to name things. “tube_wt” could be written as “Tube_wt”, “TubeWt”, “tube.wt”, “tube_weight”, etc., but as discussed we will use snake case in this course. Additionally, when was this weight collected; before or after the sample was added to the tube? 

```{r}
scfa_indv <- rename(scfa_indv, sample_wt_g = tube_wt)
```

Notes about snake case:
The general preference in the R world is to use lowercase lettering and to separate words in a name with an underscore (i.e. _). This is called “snake case”. Having a consistent capitalization strategy may seem a bit pedantic, but it makes it easier to keep the names straight when you don’t have to remember capitalization. 

We can convert the column names to lower case using the rename_all() function in the dplyr package with the tolower() function. Conversely, if you wanted everything in all caps, you could use the toupper() function. Use the rename function in the dplyr package to rename individual column names, similar to using the recode function to correct the data entry typos. 
```{r}
scfa_indv <- rename_all(scfa_indv, tolower)

colnames(scfa_indv) #check modification by printing colnames again 
```

Before proceeding, consider adding units to some of our columns. For example, you might rename the “ace” column to “acetate_mM”. Some of the modifications to column headings here are a matter of personal preference. At the end of the day consistency is most important; it makes your analysis easier to implement and share with collaborators. 

This is especially true if you have to pause the project for a few weeks or months (e.g., you go on vacation, the paper goes out for review, etc.). When you come back to it, you won’t have to recall what the abbreviated columns names represent. 

```{r}
scfa_indv <- rename(scfa_indv,
                    acetate_mM = ace, 
                    butyrate_mM = but,
                    propionate_mM = pro)

colnames(scfa_indv) #check modification by printing colnames again 
```


### Acessor functions

Take a moment to look at the columns represented in the data frame and the information presented below the column names. Do all of the values seem reasonable? Need a hint? Check out the information below “Height” and “Weight”. Think someone could weigh 0 kg or stand 0 cm tall? What about ages? To view and manipulate an individual column there are three options:

```{r}
summary(scfa_indv)

select(scfa_indv, "ht_in")
scfa_indv$ht_in
scfa_indv[["ht_in"]]
scfa_indv[[5]]
```

The first three options are pretty solid, the last option is a bit of a hassle since it requires us to count columns. Whichever approach you select, stick with it and be consistent in your coding. In this course we will primarily use the first and second approaches. These commands pull out a column from the data frame and convert it to a vector. We can use the <- operator to update our columns. The dplyr package, which is one of the core packages within the tidyverse, has a useful function called na_if. If it finds the value specified in the vector, it will convert it to an NA.

```{r}
scfa_indv$age <- na_if(scfa_indv$age, 0)

# replace na's in height and weight 

```

Running `summary(scfa_indv)` again, you see that the range for the “Age” column is more reasonable now. We’d like to look at the values for our columns that contain character values, but they’re obfuscated. One way to check this out is with the count command

```{r}
count(scfa_indv, supplement_consumed)
```

Notice anything weird here? Yup. In the “supplement_consumed” column, it looks like formatting for supplement mixes were inconsistent (some used + and other used &). We can use the dplyr function recode to make this easy...

```{r}
scfa_indv$supplement_consumed <- replace_na(scfa_indv$supplement_consumed, "none")

scfa_indv[["supplement_consumed"]] <- recode(.x=scfa_indv[["supplement_consumed"]], "BRMPS&Accessible"="BRMPS+Accessible")

count(scfa_indv, supplement_consumed) #check results by calling count again
```

Making sure the values in the data frame are correct by removing typos and ensuring they are properly bounded (e.g., no weights of zero, or concentrations 100x the biological limit) is critical to the validity of any analysis. 


# dplyr + tidyr

As introduced in the previous section, bracket subsetting can be cumbersome and difficult to read, especially for complicated operations. Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis. We’re going to learn some of the most common dplyr functions:

```{r eval=FALSE, include=FALSE}
select() #subset columns
filter() #subset rows on conditions
mutate() #create new columns by using information from other columns
group_by() %>% summarize() #create summary statistics of grouped data
arrange() #sort results
count() #count discrete values
```

### selecting columns

The first argument to this function is the data frame (scfa_wkly), and the subsequent arguments are the names of the columns to keep. To retain columns of a data frame, use select(): 

```{r}
select(scfa_indv, study_week, participant_id, butyrate_mM)
```

To retain all columns except certain ones, put a “-” in front of the variable to exclude it. This will select all the variables in scfa_wkly except use_data and notes:  

```{r}
select(scfa_indv, -use_data, -notes)
```

### filtering rows

To choose or exclude rows based on specific criteria, use filter(): 

```{r}
filter(scfa_indv, semester == "Fall2018") #choose matches
```

```{r}
filter(scfa_indv, notes != "frozen >24hrs") #exclude matches
```

### pipes

What if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, pipes.

With intermediate steps, you create a temporary data frame and use that as input to the next function.
```{r}
scfa_indv2 <- filter(scfa_indv, sample_wt_g > 0.10)
but_data <- select(scfa_indv2, 
                   participant_id, study_week, supplement_consumed, butyrate_mM, notes)
```
This is readable, but can clutter up your workspace with lots of objects that you have to name and keep track of individually. 

You can also nest functions (i.e. one function inside another).
```{r}
but_data <- select(filter(scfa_indv, sample_wt_g > 0.10), 
                   participant_id, study_week, supplement_consumed, butyrate_mM, notes)
```
This is handy, but can be difficult to read if more than a couple functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting).

The last option, pipes, are a recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like `%>%` and are made available via the magrittr package, installed automatically with tidyverse (via dyplr). If you use RStudio, you can type the pipe with the shortcut: Cmd + Shift + M on a Mac (Ctrl + Shift + M on a PC).
```{r}
scfa_indv %>%
  filter(sample_wt_g > 0.10) %>% 
  filter(notes != "frozen >24hrs") %>% 
  select(participant_id, study_week, supplement_consumed, butyrate_mM, notes)
```

In the above code, the pipe sent the scfa_indv dataset first through filter() to retain rows where sample weight is greater than 0.10, then through select() to retain only certain columns. Since %>% takes the object on its left and passes it as the first argument to the function on its right, there is no need to explicitly include the data frame as an argument to the filter() and select() functions.

Some may find it helpful to read the pipe like the word “then”. For instance, in the above example, the data frame scfa_indv, was filtered for rows with samples with a weight > 0.10, then the columns participant_id, study_week, supplement_consumed, butyrate_mM, and notes were selected. The dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, more complex manipulations of data frames are accomplished.

### Homework 3.1 
Filter the scfa_indv data frame for data from Fall 2018, only include study weeks 1 and 3, subset for BRMPS, drop any samples that do not meet the freeze time limits, or are not within the weight range. Keep columns with participant and sample data and butyrate measurements. 
```{r}
f18_but_rps <- scfa_indv %>%
```


# Calculations across columns

### mutate

Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or to find the ratio of values in two columns. For this we’ll use mutate().

To create a new column of weight in kg:
```{r}
mutate(scfa_indv, sample_kg = sample_wt_g / 1000)
```

You can also create a second new column based on the first new column within the same call of mutate():

```{r}
mutate(scfa_indv, #dataframe to use
       sample_kg = sample_wt_g / 1000, #first column modified with mutate
       acetate_mmol_kg = (acetate_mM*0.002)/sample_kg) #second column modified with mutate
```

If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as the dplyr or magrittr package is loaded).

### Homework 3.2
Do the following as one long series of commands using pipes:

* Rename columns as described in sections above (snake case, units)
* Convert measurements from US Customary to metric units (1 kg = 2.205 pounds, 1 m = 35.274 inches )
* Round participant height and weight to 0 decimals 
* Subset for samples within the weight limits 
* Subset for samples that were frozen within 24 hours
* Convert sample weights to kilograms
* Calculate mmol/kg for each SCFA
* Calculate the total SCFA in a new column 
* Round all SCFA measurments to 2 decimals
* Drop intermediate columns used for calculations 

```{r}
scfa_indv_qc <- scfa_indv %>%
```

Swap your code with your neighbor, does their code run on your computer? Are the outputs the same?

# Export data frame

After curating, extracting information, or summarising raw data, researchers often want to export these curated data sets to save them for future use or to share them with collaborators. Similar to the read_delim() function used for reading CSV or TSV files into R, there is a write_delim() function that generates files from data frames. 

Before using write_delim(), create a new folder, curated_data, in the Lab3 working directory to store this curated dataset; curated datasets should not be stored in the same directory as raw data. It’s good practice to keep them separate. As stated previously, the raw_data folder should only contain the raw, unaltered data. The code in this section will generate the contents of the curated_data directory, so the files curated_data contains can be recreated if necessary. 

```{r}
write_delim(scfa_indv_qc, path = "curated_data/scfa_indv_qc.txt", delim = "\t")
```

Remember the importance of leaving the raw data raw. Our manipulations of the scfa_indv data frame have not altered raw_data/scfa_data.xlsx. Now the cleaned up data frame is ready to share with collaborators or to use in Lab 4 next week, and all the code contained in this Rmarkdown document serves as a log of the changes made to the raw data. 

### Homework 3.3

Import the scfa_wkly data from the Excel file. Complete the following QC measures as one long series of commands using pipes:

* Rename columns to match style conventions
* Keep data from study weeks 1 or 3
* Filter for individuals who were quantity compliant
* Calculate the mean and median total SCFAs

```{r}
scfa_wkly_qc <- read_excel() %>%

```

Export this curated data frame:
```{r}
write_delim()
```


-----
end